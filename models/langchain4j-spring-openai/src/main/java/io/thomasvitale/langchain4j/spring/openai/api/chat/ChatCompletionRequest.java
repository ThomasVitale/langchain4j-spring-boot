package io.thomasvitale.langchain4j.spring.openai.api.chat;

import java.util.List;
import java.util.Map;

import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.databind.PropertyNamingStrategies;
import com.fasterxml.jackson.databind.annotation.JsonNaming;

import org.springframework.boot.context.properties.bind.ConstructorBinding;
import org.springframework.util.Assert;

/**
 * Creates a model response for the given chat conversation (POST /v1/chat/completions).
 *
 * @param messages A list of messages comprising the conversation so far.
 * @param model ID of the model to use.
 * @param frequencyPenalty Number between -2.0 and 2.0. Positive values penalize new
 * tokens based on their existing frequency in the text so far, decreasing the model's
 * likelihood to repeat the same line verbatim.
 * @param logitBias Modify the likelihood of specified tokens appearing in the completion.
 * Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer)
 * to an associated bias value from -100 to 100. Mathematically, the bias is added to the
 * logits generated by the model prior to sampling. The exact effect will vary per model,
 * but values between -1 and 1 should decrease or increase likelihood of selection; values
 * like -100 or 100 should result in a ban or exclusive selection of the relevant token.
 * @param logprobs Whether to return log probabilities of the output tokens or not. If
 * true, returns the log probabilities of each output token returned in the 'content' of
 * 'message'. This option is currently not available on the 'gpt-4-vision-preview' model.
 * @param topLogprobs An integer between 0 and 5 specifying the number of most likely
 * tokens to return at each token position, each with an associated log probability.
 * 'logprobs' must be set to 'true' if this parameter is used.
 * @param maxTokens The maximum number of tokens that can be generated in the chat
 * completion. The total length of input tokens and generated tokens is limited by the
 * model's context length.
 * @param n How many chat completion choices to generate for each input message. Note that
 * you will be charged based on the number of generated tokens across all of the choices.
 * Keep 'n' as '1' to minimize costs.
 * @param presencePenalty Number between -2.0 and 2.0. Positive values penalize new tokens
 * based on whether they appear in the text so far, increasing the model's likelihood to
 * talk about new topics.
 * @param responseFormat An object specifying the format that the model must output.
 * Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than 'gpt-3.5-turbo-1106'.
 * Setting to '{ "type": "json_object" }' enables JSON mode, which guarantees the message
 * the model generates is valid JSON.
 * @param seed This feature is in Beta. If specified, our system will make a best effort
 * to sample deterministically, such that repeated requests with the same 'seed' and
 * parameters should return the same result. Determinism is not guaranteed, and you should
 * refer to the 'system_fingerprint' response parameter to monitor changes in the backend.
 * @param stop Up to 4 sequences where the API will stop generating further tokens.
 * @param stream If set, partial message deltas will be sent, like in ChatGPT. Tokens will
 * be sent as data-only server-sent events as they become available, with the stream
 * terminated by a 'data': '[DONE]' message.
 * @param temperature What sampling temperature to use, between 0 and 2. Higher values
 * like 0.8 will make the output more random, while lower values like 0.2 will make it
 * more focused and deterministic. We generally recommend altering this or 'top_p' but not
 * both.
 * @param topP An alternative to sampling with temperature, called nucleus sampling, where
 * the model considers the results of the tokens with top_p probability mass. So 0.1 means
 * only the tokens comprising the top 10% probability mass are considered. We generally
 * recommend altering this or 'temperature' but not both.
 * @param tools A list of tools the model may call. Currently, only functions are
 * supported as a tool. Use this to provide a list of functions the model may generate
 * JSON inputs for.
 * @param toolChoice Controls which (if any) function is called by the model. 'none'
 * means the model will not call a function and instead generates a message.
 * 'auto' means the model can pick between generating a message or calling a
 * function. Specifying a particular function via '{"type": "function", "function":
 * {"name": "my_function"}}' forces the model to call that function. @{code none} is the
 * default when no functions are present. 'auto' is the default if functions are
 * present.
 * @param user A unique identifier representing your end-user, which can help OpenAI to
 * monitor and detect abuse.
 *
 * @author Thomas Vitale
 */
@JsonInclude(JsonInclude.Include.NON_NULL)
@JsonNaming(PropertyNamingStrategies.SnakeCaseStrategy.class)
public record ChatCompletionRequest(
        List<ChatCompletionMessage> messages,
        String model,
        Double frequencyPenalty,
        Map<String, Integer> logitBias,
        boolean logprobs,
        Integer topLogprobs,
        Integer maxTokens,
        Integer n,
        Double presencePenalty,
        ResponseFormat responseFormat,
        Integer seed,
        List<String> stop,
        Boolean stream,
        Double temperature,
        Double topP,
        List<Tool> tools,
        ToolChoice toolChoice,
        String user
) {

    public ChatCompletionRequest {
        Assert.notEmpty(messages, "messages must not be null or empty");
        Assert.hasText(model, "model must not be null or empty");
    }

    public static Builder builder() {
        return new Builder();
    }

    public static class Builder {
        private List<ChatCompletionMessage> messages;
        private String model;
        private Double frequencyPenalty = 0.0;
        private Map<String, Integer> logitBias;
        private boolean logprobs = false;
        private Integer topLogprobs;
        private Integer maxTokens;
        private Integer n = 1;
        private Double presencePenalty = 0.0;
        private ResponseFormat responseFormat = new ResponseFormat("text");
        private Integer seed;
        private List<String> stop;
        private Boolean stream = false;
        private Double temperature = 1.0;
        private Double topP;
        private List<Tool> tools;
        private ToolChoice toolChoice;
        private String user;

        private Builder() {}

        public Builder messages(List<ChatCompletionMessage> messages) {
            this.messages = messages;
            return this;
        }

        public Builder model(String model) {
            this.model = model;
            return this;
        }

        public Builder frequencyPenalty(Double frequencyPenalty) {
            this.frequencyPenalty = frequencyPenalty;
            return this;
        }

        public Builder logitBias(Map<String, Integer> logitBias) {
            this.logitBias = logitBias;
            return this;
        }

        public Builder logprobs(boolean logprobs) {
            this.logprobs = logprobs;
            return this;
        }

        public Builder topLogprobs(Integer topLogprobs) {
            this.topLogprobs = topLogprobs;
            return this;
        }

        public Builder maxTokens(Integer maxTokens) {
            this.maxTokens = maxTokens;
            return this;
        }

        public Builder n(Integer n) {
            this.n = n;
            return this;
        }

        public Builder presencePenalty(Double presencePenalty) {
            this.presencePenalty = presencePenalty;
            return this;
        }

        public Builder responseFormat(ResponseFormat responseFormat) {
            this.responseFormat = responseFormat;
            return this;
        }

        public Builder seed(Integer seed) {
            this.seed = seed;
            return this;
        }

        public Builder stop(List<String> stop) {
            this.stop = stop;
            return this;
        }

        public Builder stream(Boolean stream) {
            this.stream = stream;
            return this;
        }

        public Builder temperature(Double temperature) {
            this.temperature = temperature;
            return this;
        }

        public Builder topP(Double topP) {
            this.topP = topP;
            return this;
        }

        public Builder tools(List<Tool> tools) {
            this.tools = tools;
            return this;
        }

        public Builder toolChoice(ToolChoice toolChoice) {
            this.toolChoice = toolChoice;
            return this;
        }

        public Builder user(String user) {
            this.user = user;
            return this;
        }

        public ChatCompletionRequest build() {
            return new ChatCompletionRequest(messages, model, frequencyPenalty, logitBias, logprobs, topLogprobs, maxTokens, n, presencePenalty, responseFormat, seed, stop, stream, temperature, topP, tools, toolChoice, user);
        }

    }

    /**
     * Controls which (if any) function is called by the model.
     *
     * @param type The type of the tool. Currently, only 'function' is supported.
     * @param function The function to call.
     */
    @JsonInclude(JsonInclude.Include.NON_NULL)
    public record ToolChoice(
            String type,
            Function function
    ) {

        public ToolChoice {
            Assert.hasText(type, "type must not be null or empty");
            Assert.notNull(function, "function must not be null");
        }

        @ConstructorBinding
        public ToolChoice(String functionName) {
            this("function", new Function(functionName));
        }

        @JsonInclude(JsonInclude.Include.NON_NULL)
        public record Function(
                String name
        ){
            public Function {
                Assert.hasText(name, "name must not be null or empty");
            }
        }

    }

    /**
     * An object specifying the format that the model must output.
     *
     * @param type Must be one of 'text' or 'json_object'.
     */
    @JsonInclude(JsonInclude.Include.NON_NULL)
    public record ResponseFormat(String type) {
        public ResponseFormat {
            Assert.hasText(type, "type must not be null or empty");
        }
    }

}
